# openai接口说明

#### 说明
1、接口请求规范完全和openai一样，应当直接以openai的接口文档为准：https://platform.openai.com/docs/api-reference/introduction  <br>
（因为OpenAI接口也时常更新，建议直接以上面OpenAI的官方文档为准）<br>
2、支持所有的openai的接口<br>
示例：<br>
```
{
  "id": "as-bcmt5ct4iy",
  "created": 1680167072,
  "choices":[{"message":{"role":"assistant","content":"1+100=101"},"finish_reason":"stop","index":0}],
  "usage": {
    "prompt_tokens": 470,
    "completion_tokens": 198,
    "total_tokens": 668
  }
}
```

- **注意事项**
``` 
现在主要发现是有几个问题，  
1、要加一个请求头，api接口文档中有说明：
curl -H "Content-Type: application/json" -H "Authorization: Bearer 你在快AI的key" -XPOST https://api.kuaiai.app/v1/chat/completions -d '{"messages": [{"role":"user","content":"请介绍一下你自己"}]}'  | iconv -f utf-8 -t utf-8  
2、messages传的不对，messages是array
3、api_secret_key传的不对，不能再传openai的key了，你要传你从快AI拿到的key（不需要有openai的key）
4、如果是python，注意openai包的版本要对，它升级了！！要注意，关键是base_url要设置成快AI的，如果这个不正确，其它肯定都不行。所以一定要注意他在不同的包中base_url的设置方式，目前已知的是：在老版本中的设置方式是：openai.api_base = BASE_URL，而在新版本中的设置方式是：client = OpenAI(api_key=API_SECRET_KEY, base_url=BASE_URL)，别问为什么，问就是openai的锅      
```
注：<br>
1、以下所有接口的base_url: `https://api.kuaiai.app/` （支持https）<br>
2、API通过HTTP请求调用。每次请求，需要在HTTP头中携带用户的api_secret_key，用于认证。 开发者单独的api_secret_key，请从快AI管理后台获得。
请求头形如：  
```
Content-Type: application/json
Authorization: Bearer 你在快AI的key  
```

（更详细API文档地址：https://docs.kuaiai.app）
#### 示例接口1、创建chat

调用本接口，发起一次对话请求

- **请求URL**
> `v1/chat/completions`

- **请求方式** 
>**POST**

- **Header参数**
>
| 名称      |     值 | 参数说明   |
| :-------- | :--------| :------ |
| Content-Type| application/json| | 
| Authorization| Bearer 你在快AI的key| 开发者单独的api_secret_key，请从快AI管理后台获得。 | 

- **请求参数**
>
| 请求参数      |     参数类型 |   是否必须   |参数说明   |
| :-------- | :--------| :------ | :------ |
| model| string| 是| 大模型的类别，包括但不限于：gpt-4-1106-preview，gpt-4-vision-preview，gpt-3.5-turbo-1106，gpt-3.5-turbo-instruct，gpt-4, gpt-4-0314,  gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613，还有claude，gemini等，还有众多国内模型。所有模型列表（保持更新）：http://doc.zhizengzeng.com/doc-3979947|
| messages| List(message)| 是| 聊天上下文信息。说明:<br>（1）messages成员不能为空，1个成员表示单轮对话，多个成员表示多轮对话。<br>（2）最后一个message为当前请求的信息，前面的message为历史对话信息。<br>（3）必须为奇数个成员，成员中message的role必须依次为user、assistant。<br>  |
| stream| bool| 否| 是否以流式接口的形式返回数据，默认false。|
| user| string| 否| 表示最终用户的唯一标识符，可以监视和检测滥用行为，防止接口恶意调用。|

其它高级参数：
>    
| 请求参数      |     参数类型 |   是否必须   |参数说明   |  
| :-------- | :--------| :------ | :------ |  
| temperature| number| 否| What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.We generally recommend altering this or top_p but not both.，默认：1。|  
| top_p| number| 否| An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or temperature but not both.，默认：1。|  
| n| number| 否| How many chat completion choices to generate for each input message. 默认:1。|  
| stop| string| 否| Up to 4 sequences where the API will stop generating further tokens.，默认null。|
| max_tokens| number| 否| The maximum number of tokens to generate in the chat completion.The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.默认: 不限制。|
| presence_penalty| number| 否| Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.，默认:0。|
| frequency_penalty| number| 否| Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.，默认：0。|
| logit_bias| map| 否| Modify the likelihood of specified tokens appearing in the completion.Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.，默认:null。|

- **message说明**
>
| 名称      |     类型 |   描述   |
| :-------- | :--------| :------ |
| role|   string|  The role of the messages author. One of system, user, assistant, or function.<br> user: 表示用户<br>assistant: 表示对话助手<br>function：表示函数调用<br>|
| content|   string|  对话内容，不能为空。|

- **返回参数**
>
| 返回参数      |     参数类型 |   参数说明   |
| :-------- | :--------| :------ |
| code|   int|  执行结果code，0表示成功，其它表示失败，失败信息见msg字段|
| msg|   String|  执行结果消息|
| id| string| 本轮对话的id。|
| created| int| 时间戳。|
| choices| List(choice)| 对话返回结果。|
| usage| usage| token统计信息，token数 = 汉字数+单词数*1.3 （仅为估算逻辑）。|

- **choice说明**
>
| 名称      |     类型 |   描述   |
| :-------- | :--------| :------ |
| message|   message|  见上文message说明。|
| index|   int|  当前choice的序号。|
| finish_reason|   string|  结束原因。|

- **usage说明**
>
| 名称      |     类型 |   描述   |
| :-------- | :--------| :------ |
| prompt_tokens|   int|  问题tokens数。|
| completion_tokens|   int|  回答tokens数。|
| total_tokens|   int|  tokens总数。|

- **请求示例**
>    
更多示例见本页：https://github.com/xing61/xiaoyi-robot/tree/main/%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81   
```
curl -H "Content-Type: application/json" 
     -H "Authorization: Bearer $api_secret_key" 
     -XPOST https://api.kuaiai.app/v1/chat/completions -d '{
  "messages": [
    {"role":"user","content":"请介绍一下你自己"},
    {"role":"assistant","content":"您好，我是快AI机器人。我能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。"},
    {"role":"user","content": "1+100="}
  ]
}'  | iconv -f utf-8 -t utf-8
```
```
php示例代码：  
// 设置请求头
$api_secret_key = 'xxxxxxxxxxxxxxxxxx'; // 你的api_secret_key
$headers = array(
    "Content-Type: application/json",
    "Authorization: Bearer ".$api_secret_key
);
// 设置请求参数
$params = array();
$params['user'] = '张三';
{
    $one = ["role" => 'user', "content" => "1+100="];
    $messages = array(); $messages[] = $one;
    $params['messages'] = $messages;
}
// 调用请求
$cburl = 'https://api.kuaiai.app/v1/chat/completions';
$chatgpt_resp = Tool::_request('post', $cburl, $params, $headers);
$data = json_decode($chatgpt_resp, true);
```
```
python使用官方库示例代码：
import os
import openai

openai.api_key = "您的api_secret_key"
openai.base_url = "https://api.kuaiai.app/v1"  # 要注意openai的版本号，目前最新的是base_url，旧版可能是api_base

chat_completion = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{ "role": "user", "content": "Hello world" }]
)
print(chat_completion.choices[0].message.content) 
```
```
python示例代码：
import os
import requests
import time
import json

def chat_completions():
    url="https://api.kuaiai.app/v1/chat/completions"
    api_secret_key = 'xxxxxxxxx';  # 你的api_secret_key
    headers = {'Content-Type': 'application/json', 'Accept':'application/json',
               'Authorization': "Bearer "+api_secret_key}
    params = {'user':'张三',
              'messages':[{'role':'user', 'content':'1+100='}]};
    r = requests.post(url, json.dumps(params), headers=headers)
    print(r.json())

if __name__ == '__main__':
    chat_completions();
```
- **返回示例**
>    
```
{
  "id": "as-bcmt5ct4iy",
  "created": 1680167072,
  "choices":[{"message":{"role":"assistant","content":"1+100=101"},"finish_reason":"stop","index":0}],
  "usage": {
    "prompt_tokens": 470,
    "completion_tokens": 198,
    "total_tokens": 668
  }
}
```


